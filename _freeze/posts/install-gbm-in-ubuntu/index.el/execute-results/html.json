{
  "hash": "5785028aff3247883e8dae91dfabc74c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Εγκατάσταση αλγορίθμων ενίσχυσης βαθμίδας στο Linux\"\nauthor: \"stesiam\"\ndescription: \"Εγκατάσταση πακέτων - λογισμικού για τη χρήση αλγόριθμούς ενίσχυσης βαθμίδας (GBMs) (LightGBM, CatBoost & XGBoost) με την R, στο Ubuntu 22.04 LTS\"\nfreeze: false\nlink-external-newwindow: true\ncategories: [Linux, LightGBM, CatBoost, XGBoost]\nimage: images/installation_ubuntu.png\nimage-alt: \"LightGBM and CatBoost on Ubuntu 22.04\" \nfig-cap-location: bottom\ndate: \"2022-11-13\"\ntoc: true\ntoc-title: Περιεχόμενα\ntoc-location: left\ncitation: true\ntitle-block-banner: true\nnocite: '@*'\ncsl: apa-6th-edition.csl\nbibliography: references.bib\nformat: \n  html:\n    freeze: true\n    code-link: true\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-tools: \n      source: https://github.com/stesiam/stesiam.github.io/blob/gh-pages/posts/install-gbm-in-ubuntu/index.el.qmd\nexecute:\n  echo: true\neditor_options: \n  markdown: \n    wrap: 80\n---\n\n## Εισαγωγή\n\nΗ ενασχόληση με τη μηχανική μάθηση είναι ένας αρκετά ενδιαφέρον αλλά μακρύς δρόμος. Οι περισσότεροι μαθαίνουν κάποιες βασικές έννοιες και στη συνέχεια προσπαθούν να μάθουν - εφαρμόσουν με κάποιο απλό μοντέλο. Ένα από τα πρώτα μοντέλα που βλέπεις είτε σε οδηγούς εκμάθησης, είτε στη σχολή είναι η λογιστική παλινδρόμηση, γραμμική παλινδρόμηση κ.α. Αυτά είναι κάποια απλά μοντέλα τα οποία μπορούν να γίνουν αντιληπτά πιο εύκολα για το πώς δουλεούν και προσφέρουν μία αποδεκτή ακρίβεια. Πλέον αυτά τα μοντέλα σπάνια χρησιμοποιούνται από μόνα τους για να εξάγουμε κάποια συμπεράσματα, μιας και υπάρχουν αξιόλογες εναλλακτικές που μπορούν να μας δώσουν μεγαλύτερα ποσοστά ακρίβειας.  Μία από αυτές είναι οι αλγορίθμοι ενίσχυσης βαθμίδας (Gradient Boosting Machines - GBMs). Οι συγκεκριμένοι αλγόριθμοι βελτιώνουν σε μεγάλο βαθμό το μοντέλο μας και δεν είναι ιδιαίτερα δύσκολη η χρήση αυτών. Φυσικά πριν κάνουμε χρήση των μοντέλων αυτών (π.χ. με το πακέτο tidymodels), θα πρέπει να εγκαταστήσουμε τα σχετικά λογισμικά. Επομένως, σε αυτό το άρθρο θα συνοψίσω όλες τις σχετικές πληροφορίες για την εγκατάσταση αυτών. \n\n| Μοντέλο | Πηγή |\n| :---: |   :---:    |\n| LightGBM |  [Σύνδεσμος](https://lightgbm.readthedocs.io/en/v3.3.2/Installation-Guide.html)  |\n| CatBoost |  [Σύνδεσμος](https://catboost.ai/en/docs/concepts/installation)  |\n| XGBoost  |  [Σύνδεσμος](https://xgboost.readthedocs.io/en/stable/install.html)  |\n: Αλγόριθμοί Boosting και ιστοσελίδες τεκμηρίωσης {#tbl-documentation-list-gbm}\n\n\n::: {.cell .fig-cap-location-bottom}\n\n```{.r .cell-code}\nlibrary(highcharter)\nlibrary(gtrendsR)\nlibrary(dplyr)\ngoogleTrendsData = gtrendsR::gtrends(keyword = c(\"LightGBM\", \"CatBoost\", \"XGBoost\"), gprop = \"web\", onlyInterest = TRUE)\n\ninterestOverTime = googleTrendsData[[\"interest_over_time\"]] %>%\n  dplyr::mutate(date = lubridate::ymd(date)) %>%\n  dplyr::mutate(Year = lubridate::year(date)) %>%\n  select(Year, keyword, hits) %>%\n  group_by(Year, keyword) %>%\n  summarise(Average = round(mean(hits), digits = 1))\n  \n\nhighchart() %>%\n    hc_chart(type = \"line\") %>%\n    hc_title(text = \"Ενδιαφέρον αναζήτησης σε μηχανές αναζήτησης\") %>%\n    hc_subtitle(text = \"Ποιος από τους αλγόριθμούς ενίσχυσης βαθμίδας (GBMs) έχει περισσότερες\n                αναζητήσεις.\") %>%\n    hc_xAxis(categories = unique(interestOverTime$Year)) %>%\n    hc_yAxis(title = list(text = \"Τάση\")) %>%\n    hc_add_series(\n        name = \"XGBoost\",\n        data = interestOverTime %>% filter(keyword == \"XGBoost\") %>% pull(Average)\n    ) %>%\n    hc_add_series(\n        name = \"CatBoost\",\n        data = interestOverTime %>% filter(keyword == \"CatBoost\") %>% pull(Average)\n    ) %>%\n    hc_add_series(\n        name = \"LightGBM\",\n        data = interestOverTime %>% filter(keyword == \"LightGBM\") %>% pull(Average)\n    )\n```\n:::\n\n\n## LightGBM\n\n### Επιλογή 1. Εγκατάσταση του πακέτου R\n\nΑν διαβάζεις αυτή την ιστοσελίδα πιθανότατα γνωρίζεις ήδη ότι προγραμματίζω κυρίως με R. Είναι προφανές ότι θα ξεκινήσω με μία λύση για τους R χρήστες. Στη συγκεκριμένη περίπτωση εγκαθιστώ το αντίστοιχο πακέτο της R, lightgbm, έτσι ώστε να το χρησιμοποιήσουμε.\n\n```{.r filename=\"R code\"}\nstart_time_lightgbm <- Sys.time()\ninstall.packages(\"lightgbm\", repos = \"https://cran.r-project.org\")\nend_time_lightgbm <- Sys.time()\n```\n\n\n### Επιλογή 2. Εγκατάσταση μέσω του CMAKE\n\nΣτη σελίδα [τεκμηρίωσης](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html#linux) του LightGBM αναφέρονται περισσότερες λεπτομέρειες, ωστόσο οι βασικές εντολές για εγκατάσταση μέσω του terminal είναι οι εξής:\n\n```{.bash filename=\"Terminal\"}\nsudo apt install cmake\n```\n\n```{.bash filename=\"Terminal\"}\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nmkdir build\ncd build\ncmake ..\nmake -j4\n```\n\n\n## CatBoost\n\nΓια το CatBoost μπορείτε να βρείτε [εδώ τις εκδόσεις του λογισμικού](https://github.com/catboost/catboost/releases). Στην δικιά μου περίπτωση όταν προσπάθησα να εγκαταστήσω τη βιβλιοθήκη `devtools` προέκυψε ένα σφάλμα που δεν μου επέτρεψε την εγκατάσταση του πακέτου. Σύμφωνα με το μήνυμα σφάλματός στον υπολογιστή μου (στον οποίο τρέχω Ubuntu 22) μου έλειπαν οι βιβλιοθήκες `libharfbuzz-dev` και `libfribidi-dev`. Αφού πρόσθεσα τα παραπάνω πακέτα και επανεκκίνησα το RStudio, η εγκατάσταση του `devtools` ολοκληρώθηκε κανονικά. Αφού εγκαταστήσψ το devtools μπορώ να χρησιμοποιήσω την εντολή `install_url`, καθώς δεν υπάρχει (ή τουλάχιστον δεν γνωρίζω) πακέτο αντίστοιχο του lightgbm. Έτσι χρησιμοποιώ έναν σύνδεσμο αναλόγως της έκδοσης που χρειάζομαι από την επίσημη σελίδα του CatBoost project στο GitHub.\n\n![](images/error_message_devtools_install.png) \n\n```{.r filename=\"R code\"}\nstart_time_catboost <- Sys.time()\ndevtools::install_url(\"https://github.com/catboost/catboost/releases/download/v1.1.1/catboost-R-Linux-1.1.1.tgz\"[, INSTALL_opts = c(\"--no-multiarch\", \"--no-test-load\")])\nend_time_catboost <- Sys.time()\n```\n\n\n## XGBoost\n\nΤέλος, μας έχει απομείνει ίσως το πιο γνωστό πακέτο μεταξύ των gradient boosting. Η συγκεκριμένη υλοποίηση είναι ίσως αυτή που βελτιώνει περισσότερο τις εκτιμήσεις μας σε μεθόδους μηχανικής μάθησης. Σταθερά σε πάρα πολλά προβλήματα ξεπερνάει σε απόδοση τις υπόλοιπες συμβατές μεθόδους (λογιστική / γραμμική παλινδρόμηση, Naive Bayes κτλ.). Βέβαια όταν προσπαθήσω να κατασκευάσω ένα μοντέλο μηχανικής μάθησης με το XGBoost, ενδεχομένως να είναι το πιο χρονοβόρο σε σχέση με τις υπόλοιπες υλοποιήσεις. Η εγκατάσταση του πακέτου είναι αντίστοιχα εύκολη με αυτή του lightgbm, μέσω του ομόνυμου πακέτου, xgboost.\n\n\n```{.r filename=\"R code\"}\nstart_time_xgboost <- Sys.time()\ninstall.packages(\"xgboost\")\nend_time_xgboost <- Sys.time()\n```\n\n\n\n## Σύνοψη \n\nΟι αλγόριθμοι ενίσχυσης βαθμίδας μας δίνουν προβλεπτικά μοντέλα με αυξημένη απόδοση (προβλεπτική ικανότητα) σε σχέση με άλλες συμβατικές μεθόδους μηχανικής μάθησης. Σε αυτό το άρθρο διαπιστώθηκε ότι η R κατά κύριο λόγο έχει εύκολους τρόπους για να εγκαταστήσουμε και εν τέλει να χρησιμοποιήσουμε. Κλείνοντας, θα ήθελα να κλείσω με ένα μικρό πείραμα που έκανα. Παρατήρησα ότι η εγκατάσταση αυτών των πακέτων είναι ιδιαίτερα χρονοβόρα και σκέφτηκα να χρονομετρήσω τη διάρκεια για την εγκατάστασή τους. Έτσι λοιπόν, περισσότερη ώρα πήρε το πακέτο LightGBM, κάνοντας σχεδόν 8 λεπτά, ακολουθούμενο από το XGBoost, και τέλος το CatBoost για το οποίο διήρκησε μόλις 2 λεπτά!\n\n:::{.table-responsive}\n| Μοντέλο | Μέθοδος εγκατάστασης |  Χρόνος εγκατάστασης |\n| :---: | :---: | :---:    |\n| LightGBM | R πακέτο  | 7.79 λεπτά  |\n| CatBoost | R πακέτο (χωρίς devtools)  | 2.1 λεπτά  |\n| XGBoost  | R πακέτο  |6.16 λεπτά |\n: Χρόνοι εγκατάστασης γνωστών αλγορίθμων ενίσχυσης βαθμίδας {#tbl-install-times}\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}